# TextArena integration

This folder contains the instance generator, game master, scorer, and game specs necessary to integrate games from [TextArena](https://github.com/LeonGuertler/TextArena/tree/main) into the clem framework.

## Structure

* `master.py`: Only contains the GameBenchmark
* [ta_master.py](#gamemaster)
    * Contains the actual `TextArenaPlayer` and `TextArenaGameMaster` classes. The latter is implemented as similar as possible to `DialogueGameMaster`, and offers a number of convenience functions for sub-classing.
    * Holds most of the integration functionality. For example, `TextArenaGameMaster.step()` will both advance the TextArena environment and handle the proper clem-style logging.
    * Model calls use `TextArenaPlayer` only.
* `clem_observation_wrapper.py`: `ClemObservationWrapper` is a subclass of TextArena's `ObservationWrapper` and mostly takes care of converting the messages generated by the environment into the appropriate context format used by clem.
* [submasters.py](#gamemaster): Holds subclasses of `TextArenaGameMaster` for either individual games, such as `WordChainsMaster`, or groups of games, such as `SinglePlayerMaster`. Subclassing is mostly necessary for two reasons:
    * To ensure deterministic sampling, see [Reproducibility](#reproducibility)
    * To extract information from the TA environment before it closes, see [Logging from ta_env](#logging-taenv-values)
* [metrics.py](#scoring): Holds `GameScorer` subclasses with the main purpose of calculating the bench score
* [clemgame.json](#clemgamejson): holds game definitions and configurations. References both the TA entry point and the suitable `GameMaster` and  `GameScorer` classes as well as  mock player responses.
* `instancegenerator.py`: Makes sure that all difficulty levels offered by TA (or custom experiments) get their own instance(s).


## Adding games

### TextArena structure

In the TextArena repo, the game environments containing all game logic are stored in `textarena/envs`. Each env inherits from `ta.Env`, and has a game `ta.State` depending on the number of players. They can be wrapped in different ObservationWrappers for frontends or response generation.

### `clemgame.json`

Check [(link)](https://www.textarena.ai/environments/) for fully implemented games (also `TextArena/textarena/envs/__init__.py` [(link)](https://github.com/LeonGuertler/TextArena/blob/main/textarena/envs/__init__.py), but some of them are unfinished).
Once you have chosen a game, you can make a new entry in `clemgamge.json` of the structure:

```
{
    "game_name": "ta_minesweeper",
    "entry_point": "textarena.envs.Minesweeper.env:MinesweeperEnv",
    "master": "MinesweeperMaster",
    "scorer": "SinglePlayerScorer",
    "n_instances": 10,
    "instances": "instances_minesweeper",
    "players": 1,
    "image": null,
    "languages": [
        "en"
    ],
    "player_specs": [
        {
            "role": "Minesweeper",
            "custom_response": ["[3 2]", "[5 6]"]
        }
    ]
}
```

* `game_name` should start with `ta_`. 
* `entry_point` references the TA Env to be used
* `master` and `scorer`: The GameMaster and Scorer subclasses to be used
* `n_instances` is the number of instances per experiment
* `instances` is the name of the resulting json file, should always start with `instances_`.
* `n_players` must match the number of players TA requires, cf. [envs Readme](https://github.com/LeonGuertler/TextArena/blob/main/textarena/envs/README.md)
* always add `"image": null` and `"languages": ["en"]`; as of August 2025, TA neither supports neither multimodal input nor other languages.
* `player specs`: Must match `n_players`. For role, you can use descriptive names such us `Guesser` etc. `custom_response` is for programmatic answers that are sampled from when playing with `-m mock`.

#### Experiments

TA defines experiments with different difficulty levels, e.g.:
`"Minesweeper-v0" (default), "Minesweeper-v0-small", "Minesweeper-v0-medium", "Minesweeper-v0-hard"`
By default, all experiments specified by TA are used, but you can also provide a list in the `experiments` key in `clemgame.json`, such as:
```
    "experiments": ["default", "small", "medium"]
```


Alternatively, you can provide a dict with experiments and configs such as:
```
"game_name": "ta_battleship",
"entry_point": "textarena.envs.Battleship.env:BattleshipEnv",
"experiments": {
    "small": {
            "grid_size": 7
    },
    "standard": {
            "grid_size": 10
    }
},
[...]
```

## GameMaster

Only subclass `GameMaster` if strictly necessary. For example, most single player games are playable with `SinglePlayerMaster`. Reasons for subclassing are mostly reproducibility, and logging values from the env for scoring.

There are three standard hooks that can be used to add functionality:
* `_on_before_reset()`, which is called after the env is created, but before the game is initialized,
* `_on_before_game()`, which is called after the game is set up,
* and the abstract method `_on_after_game()`, which is called with the rewards passed from TA. Subclasses should log `METRIC_ABORT`, `METRIC_SUCCESS`, and `METRIC_LOSE` here.

### Reproducibility

Despite passing the seed, there might occur some non-deterministic behavior, which might be fixed by subclassing `GameMaster`.

For example, in WordChains, the start word is sampled from a list generated from a set, which is non-deterministic. So, `WordChainsMaster` sorts it in `_on_before_reset()`.

`Minesweeper` only distributes the mines after the first move. Thus, `_on_before_game()` simulates the first move, which is always opening the most central cell.

### Logging ta_env values

All values needed for scoring or evaluation must be extracted from the env before it closes.
For example, `WordChainsMaster` logs `start_word` and `start_word_length` in `_on_before_game()`, and `end_word`, `end_word_length` and `word_length_diff` in `_on_after_game`.

## Scoring

Scoring is somewhat difficult and depends on the game and number of players. `rewards = env.close()` usually gives a good starting point.

### Single Player

In single player games, `rewards` contains a numeric value that usually takes on of the following values:
* `-1` means aborted (NOTE: as of August 2025, single player games never return `-1`, likely a bug)
* `1` means success
* a float in the interval `[0-1]`, e.g. if a game was terminated by exceeding turn limit. Usually calculated by a `ta_env._get_percentage_completion()`
In the latter two cases, the reward can be the basis for calculating the Main Score.

`SinglePlayerScorer` simply logs `reward * 100` as Main Score (if `>=0`).


### Multiplayer

Some games, such as Word Chains, can be interpreted as collaborative, so we can base scoring on the length of the final word in the chain.

Most games, however, are competitive and only give the rewards win (`1`) or lose (`-1`).
TODO: Implement robust Multiplayer scoring


# Notes on individual games

## Word Chains

Scoring is based on the achieved final word length. Only 66 of the 263,689 (.025%) words in the utilized dictionary are longer than 21 letters, so this is a reasonable upper limit for a perfect game.

Main Score is then simply the percentage of achieved word length to upper word length (capped at `100`).

## Hangman

Scoring is based on percentage of correctly guessed letters.
Remaining lives are factored in the following way:
* `life_score` is $\frac{l}{2 \times L} + \frac{1}{2}$, where $l$ is the number of lives left, and $L$ the initial number. It is always in the interval $[0.5, 1]$
* Multiplied with percentage of guessed letters